{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis\n",
       "B    357\n",
       "M    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing dataset from UCI ML Repository\n",
    "import pandas as pd \n",
    "\n",
    "# Using UCI ML Repository because load_breast_cancer in sklearn.datasets doesn't include 'diagnosis'\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\"\n",
    "column_names = [\"id\", \"diagnosis\", \"radius_mean\", \"texture_mean\", \"perimeter_mean\", \"area_mean\", \"smoothness_mean\", \"compactness_mean\", \"concavity_mean\", \"concave points_mean\", \"symmetry_mean\", \"fractal_dimension_mean\", \"radius_se\", \"texture_se\", \"perimeter_se\", \"area_se\", \"smoothness_se\", \"compactness_se\", \"concavity_se\", \"concave points_se\", \"symmetry_se\", \"fractal_dimension_se\", \"radius_worst\", \"texture_worst\", \"perimeter_worst\", \"area_worst\", \"smoothness_worst\", \"compactness_worst\", \"concavity_worst\", \"concave points_worst\", \"symmetry_worst\", \"fractal_dimension_worst\"]\n",
    "\n",
    "df = pd.read_csv(url, names=column_names)\n",
    "\n",
    "df.head()\n",
    "df.columns\n",
    "df['diagnosis'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B' 'M']\n",
      "Model accuracy: 0.9298245614035088\n",
      "Logistic Regression training and prediction time: 0.018082857131958008 seconds\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        71\n",
      "           1       0.89      0.93      0.91        43\n",
      "\n",
      "    accuracy                           0.93       114\n",
      "   macro avg       0.92      0.93      0.93       114\n",
      "weighted avg       0.93      0.93      0.93       114\n",
      "\n",
      "ROC AUC score: 0.9299050114641335\n",
      "\n",
      "Feature: radius_mean, Score: -2.39880795219185\n",
      "Feature: texture_mean, Score: 0.23377629281415488\n",
      "Feature: perimeter_mean, Score: 0.5665869622072763\n",
      "Feature: area_mean, Score: -0.0047376948950455\n",
      "Feature: smoothness_mean, Score: 0.4228157935522469\n",
      "Feature: compactness_mean, Score: 0.7167695646330422\n",
      "Feature: concavity_mean, Score: 1.2669748455037906\n",
      "Feature: concave points_mean, Score: 0.6788174183189047\n",
      "Feature: symmetry_mean, Score: 0.5992161591717076\n",
      "Feature: fractal_dimension_mean, Score: 0.12209526928113215\n",
      "\n",
      "Most positively influential feature on diagnosis with malignant mass: concavity_mean\n",
      "Most negatively influential feature on diagnosis with malignant mass: radius_mean\n"
     ]
    }
   ],
   "source": [
    "# 1. Logistic regression\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean', 'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean']\n",
    "X = df[features]\n",
    "y = df['diagnosis']\n",
    "\n",
    "# Encoding the target variable (B = 0, M = 1, as in the original dataset)\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "print(le.classes_)\n",
    "\n",
    "# 80% for training \n",
    "X_train_lgr, X_test_lgr, y_train_lgr, y_test_lgr = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=10000)\n",
    "\n",
    "# Starting the timer to calculate training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Training\n",
    "model.fit(X_train_lgr, y_train_lgr)\n",
    "\n",
    "y_pred_lgr = model.predict(X_test_lgr)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "tnp_time_lgr = end_time - start_time\n",
    "\n",
    "accuracy_lgr = accuracy_score(y_test_lgr, y_pred_lgr)\n",
    "\n",
    "print(f'Model accuracy: {accuracy_lgr}')\n",
    "print(f'Logistic Regression training and prediction time: {tnp_time_lgr} seconds')\n",
    "\n",
    "# Generating classification report\n",
    "report = classification_report(y_test_lgr, y_pred_lgr)\n",
    "print(report)\n",
    "\n",
    "roc_auc = roc_auc_score(y_test_lgr, y_pred_lgr)\n",
    "print(f'ROC AUC score: {roc_auc}\\n')\n",
    "\n",
    "# Getting the feature importance\n",
    "importance = model.coef_[0]\n",
    "\n",
    "# Summarizing feature importance\n",
    "for i, j in enumerate(importance):\n",
    "    print(f'Feature: {features[i]}, Score: {j}')\n",
    "\n",
    "print()\n",
    "\n",
    "max_positive_index = importance.argmax()\n",
    "max_negative_index = importance.argmin()\n",
    "\n",
    "print(f'Most positively influential feature on diagnosis with malignant mass: {features[max_positive_index]}')\n",
    "print(f'Most negatively influential feature on diagnosis with malignant mass: {features[max_negative_index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model accuracy = Number of Correct Predictions​ / Total Number of Predictions.** (This value was calculated with a separate function.)\n",
    "The model has a big accuracy rate (0.93).\n",
    "\n",
    "**Precision = True Positives / True Positives + False Positives​,** is THE RATIO OF TRUE POSITIVES to the sum of true positives and false positives. It indicates how many of the positive predictions were ACTUALLY correct. \n",
    "High precision for both classes indicates that the model performing well. \n",
    "The model seems to make predictions with high success in both classes, slightly better in the 0 class (0.96) than in the 1 class (0.89).\n",
    "\n",
    "**Recall = True Positives​ / True Positives + False Negatives,** also known as sensitivity or true positive rate, measures the proportion of actual positive cases that were correctly identified by the model. \n",
    "It answers the question: \"Of all the actual positives, how many did the model correctly predict?\" \n",
    "High Recall indicates that the model is good at identifying all positive instances, but it might also lead to more false positives. \n",
    "The model shows 93% success rate for both classes.\n",
    "\n",
    "**F1-score = 2 x (Precision x Recall / Precision + Recall)​,** is the harmonic mean of precision and recall, providing a single metric that balances both. \n",
    "High F1-scores for both classes indicate a good balance between precision and recall and good model performance. \n",
    "The model has a high F1 Score for both classes, with class 0 (0.94) being slightly higher than class 1 (0.91).\n",
    "\n",
    "**Support** is the number of actual occurrences of the classes. It gives context to the other metrics. It is 71 for class 0 and 43 for class 1.\n",
    "\n",
    "**Accuracy** is the ratio of correctly predicted instances to the total instances. (This value was calculated in the classification report)\n",
    "It provides an overall indication of how often the model is correct.\n",
    "The model has an overall success rate of 93%.\n",
    "\n",
    "**Macro avg** is the average of the precision, recall, and F1-score across all classes, weighted equally. It provides an overall metric of the model's performance. The model has high macro avg values (0.92 for precision, 0.93 for recall and F1 score). \n",
    "\n",
    "**The weighted average** takes into account the support (number of true instances) for each class, providing a better overall metric for imbalanced datasets. The model also has high weighted average values (0.93 for all metrics).\n",
    "\n",
    "**ROC AUC** is a measure of the model's ability to discriminate between positive and negative classes.\n",
    "It is a numerical expression of ROC Curve and Area Under the Curve.\n",
    "The model is capable of distinguishing between the positive class and the negative class with 93% accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree training and prediction time: 0.006905078887939453 seconds\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        71\n",
      "           1       0.88      0.88      0.88        43\n",
      "\n",
      "    accuracy                           0.91       114\n",
      "   macro avg       0.91      0.91      0.91       114\n",
      "weighted avg       0.91      0.91      0.91       114\n",
      "\n",
      "ROC AUC score: 0.9066491975106452\n",
      "\n",
      "Feature: radius_mean, Score: 0.0018210283843930922\n",
      "Feature: texture_mean, Score: 0.07410236185108188\n",
      "Feature: perimeter_mean, Score: 0.023858157823691654\n",
      "Feature: area_mean, Score: 0.05970311938155739\n",
      "Feature: smoothness_mean, Score: 0.019377589823598745\n",
      "Feature: compactness_mean, Score: 0.01775147928994082\n",
      "Feature: concavity_mean, Score: 0.04275409598376106\n",
      "Feature: concave points_mean, Score: 0.7496495603255221\n",
      "Feature: symmetry_mean, Score: 0.010982607136453288\n",
      "Feature: fractal_dimension_mean, Score: 0.0\n",
      "\n",
      "Most influential feature on diagnosis with malignant mass: concave points_mean\n",
      "Least influential feature on diagnosis with malignant mass: fractal_dimension_mean\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" In the decision tree, these values are not interpreted as coefficients. \\nInstead, they indicate the importance of each feature in determining the model's decisions. \\nThe influence of features is not separated as positive or negative. \\nTherefore, the least influential feature may still have an influence, \\nbut this influence is less than that of other features. \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(44)\n",
    "\n",
    "# Creating a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "X_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf.fit(X_train_dt, y_train_dt)\n",
    "\n",
    "y_pred_dt = clf.predict(X_test_dt) # Making predictions on the test set\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "tnp_time_dt = end_time - start_time\n",
    "print(f'Decision Tree training and prediction time: {tnp_time_dt} seconds\\n')\n",
    "\n",
    "report = classification_report(y_test_dt, y_pred_dt)\n",
    "print(report)\n",
    "\n",
    "roc_auc_dt = roc_auc_score(y_test_dt, y_pred_dt)\n",
    "print(f'ROC AUC score: {roc_auc_dt}\\n')\n",
    "\n",
    "# Feature importances\n",
    "importance_dt = clf.feature_importances_\n",
    "\n",
    "for i, j in enumerate(importance_dt):\n",
    "    print(f'Feature: {features[i]}, Score: {j}')\n",
    "\n",
    "print()\n",
    "\n",
    "max_positive_index_dt = importance_dt.argmax()\n",
    "min_positive_index_dt = importance_dt.argmin()\n",
    "\n",
    "print(f'Most influential feature on diagnosis with malignant mass: {features[max_positive_index_dt]}')\n",
    "print(f'Least influential feature on diagnosis with malignant mass: {features[min_positive_index_dt]}')\n",
    "\n",
    "\"\"\" \n",
    "In the decision tree, these values are not interpreted as coefficients. \n",
    "Instead, they indicate the importance of each feature in determining the model's decisions. \n",
    "The influence of features is not separated as positive or negative. \n",
    "Therefore, the least influential feature may still have an influence, \n",
    "but this influence is less than that of other features. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decision Tree model has a shorter training and testing time. \n",
    "It may be advantageous to use a Decision Tree when working with large data sets or in projects where fast prediction is required. \n",
    "Nevertheless, according to the classification report, the Logistic Regression model performs slightly better in many metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest training and prediction time: 0.1474318504333496 seconds\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97        71\n",
      "           1       0.95      0.93      0.94        43\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "Random Forest ROC AUC score: 0.9510317720275139\n",
      "\n",
      "Feature: radius_mean, Score: 0.1277548816015485\n",
      "Feature: texture_mean, Score: 0.05761965781797203\n",
      "Feature: perimeter_mean, Score: 0.1419629655393132\n",
      "Feature: area_mean, Score: 0.1462915048546199\n",
      "Feature: smoothness_mean, Score: 0.02584276927626497\n",
      "Feature: compactness_mean, Score: 0.04889031192818518\n",
      "Feature: concavity_mean, Score: 0.14695627090912988\n",
      "Feature: concave points_mean, Score: 0.26585529642294686\n",
      "Feature: symmetry_mean, Score: 0.019716685733047416\n",
      "Feature: fractal_dimension_mean, Score: 0.01910965591697205\n",
      "\n",
      "Most influential feature on diagnosis with malignant mass: concave points_mean\n",
      "Least influential feature on diagnosis with malignant mass: fractal_dimension_mean\n"
     ]
    }
   ],
   "source": [
    "# 3. Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(555)\n",
    "\n",
    "# Creating a Random Forest classifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "start_time_rf = time.time()\n",
    "\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf_rf.fit(X_train_rf, y_train_rf)\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test_rf)\n",
    "\n",
    "end_time_rf = time.time()\n",
    "\n",
    "tnp_time_rf = end_time_rf - start_time_rf\n",
    "print(f'Random Forest training and prediction time: {tnp_time_rf} seconds\\n')\n",
    "\n",
    "report_rf = classification_report(y_test_rf, y_pred_rf)\n",
    "print(report_rf)\n",
    "\n",
    "roc_auc_rf = roc_auc_score(y_test_rf, y_pred_rf)\n",
    "print(f'Random Forest ROC AUC score: {roc_auc_rf}\\n')\n",
    "\n",
    "importance_rf = clf_rf.feature_importances_\n",
    "\n",
    "for i, j in enumerate(importance_rf):\n",
    "    print(f'Feature: {features[i]}, Score: {j}')\n",
    "\n",
    "print()\n",
    "\n",
    "max_importance_index_rf = importance_rf.argmax()\n",
    "min_importance_index_rf = importance_rf.argmin()\n",
    "\n",
    "print(f'Most influential feature on diagnosis with malignant mass: {features[max_importance_index_rf]}')\n",
    "print(f'Least influential feature on diagnosis with malignant mass: {features[min_importance_index_rf]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Random Forest model has a longer training and testing time than the other two models, it has a higher accuracy rate and higher performance metric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Machine training and prediction time: 0.008525609970092773 seconds\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98        71\n",
      "           1       1.00      0.93      0.96        43\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.98      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "SVM ROC AUC score: 0.9651162790697674\n",
      "\n",
      "Feature: radius_mean, Score: 0.1277548816015485\n",
      "Feature: texture_mean, Score: 0.05761965781797203\n",
      "Feature: perimeter_mean, Score: 0.1419629655393132\n",
      "Feature: area_mean, Score: 0.1462915048546199\n",
      "Feature: smoothness_mean, Score: 0.02584276927626497\n",
      "Feature: compactness_mean, Score: 0.04889031192818518\n",
      "Feature: concavity_mean, Score: 0.14695627090912988\n",
      "Feature: concave points_mean, Score: 0.26585529642294686\n",
      "Feature: symmetry_mean, Score: 0.019716685733047416\n",
      "Feature: fractal_dimension_mean, Score: 0.01910965591697205\n",
      "\n",
      "Most influential feature on diagnosis with malignant mass: concave points_mean\n",
      "Least influential feature on diagnosis with malignant mass: fractal_dimension_mean\n"
     ]
    }
   ],
   "source": [
    "# 4. Support Vector Machine\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "start_time_svm = time.time()\n",
    "\n",
    "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scaling the features to ensure that all features have the same scales\n",
    "scaler = StandardScaler()\n",
    "X_train_svm = scaler.fit_transform(X_train_svm)\n",
    "X_test_svm = scaler.transform(X_test_svm)\n",
    "\n",
    "'''\n",
    "fit + transform; fit calculates the parameters (mean and standard deviation) needed to standardise the data. \n",
    "transform standardises the TRAINING DATA using these parameters.\n",
    "fit_transform performs these two operations in a single step. \n",
    "\n",
    "transform; standardises the TEST DATA using the parameters calculated from the training data.\n",
    "\n",
    "The model learns based on the distribution of the Training Data and \n",
    "therefore the Test Data should use the same distribution when evaluating the model.\n",
    "\n",
    "In distance-based algorithms like KNN or SVM, if features are not on the same scale, \n",
    "features with larger scales may dominate the model, leading to suboptimal performance.\n",
    "'''\n",
    "\n",
    "# Creating an SVM model\n",
    "svm_model = SVC()\n",
    "\n",
    "svm_model.fit(X_train_svm, y_train_svm)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_svm)\n",
    "\n",
    "end_time_svm = time.time()\n",
    "\n",
    "tnp_time_svm = end_time_svm - start_time_svm\n",
    "print(f'Support Vector Machine training and prediction time: {tnp_time_svm} seconds\\n')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_svm, y_pred_svm))\n",
    "\n",
    "roc_auc_svm = roc_auc_score(y_test_svm, y_pred_svm)\n",
    "print(f'SVM ROC AUC score: {roc_auc_svm}\\n')\n",
    "\n",
    "importance_rf = clf_rf.feature_importances_\n",
    "\n",
    "for i, j in enumerate(importance_rf):\n",
    "    print(f'Feature: {features[i]}, Score: {j}')\n",
    "\n",
    "print()\n",
    "\n",
    "max_importance_index_rf = importance_rf.argmax()\n",
    "min_importance_index_rf = importance_rf.argmin()\n",
    "\n",
    "print(f'Most influential feature on diagnosis with malignant mass: {features[max_importance_index_rf]}')\n",
    "print(f'Least influential feature on diagnosis with malignant mass: {features[min_importance_index_rf]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Support Vector Machine model has short training and testing time and high performance metrics and is the best model in terms of speed and accuracy out of the 4 models so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors training and prediction time: 0.011308431625366211 seconds\n",
      "\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        72\n",
      "           1       0.95      0.90      0.93        42\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.95      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "KNN ROC AUC score: 0.9384920634920634\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: symmetry_mean, score: 0.021052631578947323\n",
      "Feature: fractal_dimension_mean, score: 0.03508771929824557\n",
      "Feature: compactness_mean, score: 0.036842105263157864\n",
      "Feature: texture_mean, score: 0.04298245614035082\n",
      "Feature: smoothness_mean, score: 0.047368421052631525\n",
      "Feature: concavity_mean, score: 0.049122807017543804\n",
      "Feature: concave points_mean, score: 0.06315789473684205\n",
      "\n",
      "Most influential feature on diagnosis: concave points_mean\n",
      "Least influential feature on diagnosis: symmetry_mean\n"
     ]
    }
   ],
   "source": [
    "# 5. K-Nearest Neighbors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "start_time_knn = time.time()\n",
    "\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# Scaling the features\n",
    "scaler = StandardScaler()\n",
    "X_train_knn = scaler.fit_transform(X_train_knn)\n",
    "X_test_knn = scaler.transform(X_test_knn)\n",
    "\n",
    "# Creating and training the KNN classifier\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_knn, y_train_knn)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred_knn = knn_model.predict(X_test_knn)\n",
    "\n",
    "end_time_knn = time.time()\n",
    "\n",
    "# Evaluating the model\n",
    "tnp_time_knn = end_time_knn - start_time_knn\n",
    "print(f'K-Nearest Neighbors training and prediction time: {tnp_time_knn} seconds\\n')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_knn, y_pred_knn))\n",
    "\n",
    "roc_auc_knn = roc_auc_score(y_test_knn, y_pred_knn)\n",
    "print(f'KNN ROC AUC score: {roc_auc_knn}\\n')\n",
    "\n",
    "from sklearn.inspection import permutation_importance # To get the feature importance\n",
    "\n",
    "result = permutation_importance(knn_model, X_test_knn, y_test_knn, n_repeats=10, random_state=42, n_jobs=-1)\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "importance_scores = {}\n",
    "\n",
    "for i in sorted_idx:\n",
    "    if result.importances_mean[i] - 2 * result.importances_std[i] > 0:\n",
    "        importance_scores[features[i]] = result.importances_mean[i]\n",
    "        print(f\"Feature: {features[i]}, score: {result.importances_mean[i]}\")\n",
    "\n",
    "# Sorting the dictionary by value in descending order\n",
    "importance_scores_sorted = dict(sorted(importance_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Getting the most and least influential features\n",
    "most_influential_feature = list(importance_scores_sorted.keys())[0]\n",
    "least_influential_feature = list(importance_scores_sorted.keys())[-1]\n",
    "\n",
    "print(f\"\\nMost influential feature on diagnosis: {most_influential_feature}\")\n",
    "print(f\"Least influential feature on diagnosis: {least_influential_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbours model is almost as fast as the Support Vector Machine model, but performs worse than the Random Forest Model in many performance metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall ranking\n",
    "\n",
    "### Accuracy\n",
    "1.  SVM (0.97)\n",
    "2.  Random Forest (0.96)\n",
    "3.  KNN (0.95)\n",
    "4.  Logistic Regression (0.93)\n",
    "5.  Decision Tree (0.91)\n",
    "\n",
    "### Training and Prediction Time\n",
    "1.  Decision Tree (0.0069 seconds)\n",
    "2.  SVM (0.0085 seconds)\n",
    "3.  KNN (0.0113 seconds)\n",
    "4.  Logistic Regression (0.0180 seconds)\n",
    "5.  Random Forest (0.1474 seconds)\n",
    "\n",
    "### ROC AUC Scores\n",
    "1.  SVM (0.9651)\n",
    "2.  Random Forest (0.9510)\n",
    "3.  KNN (0.9384)\n",
    "4.  Logistic Regression (0.9299)\n",
    "5.  Decision Tree (0.9066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final results\n",
    "\n",
    "The results of the models in terms of tumour features show that **the features related to the concavity of the tumour** (concave points_mean, concavity_mean) are the features that have **the greatest impact on diagnosis.** \n",
    "\n",
    "**SVM**  emerges as the **best performing algorithm.** It achieves the highest accuracy (0.97) and the highest ROC AUC score (0.9651), while maintaining a competitive training and prediction time (0.0085 seconds), ranking second in case of time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
